{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from s3fs import S3FileSystem\n",
    "import json\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "mn=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c08757",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc=\"path\\to\\bitcoin.csv\"\n",
    "data=pd.read_csv(file_loc,header=0)\n",
    "#rename the columns \n",
    "data.columns=[\"DateTime\",\"Price\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition the first 80% of rows for traind dataset and 20% for testing process\n",
    "partitions=int(len(data)*0.8)\n",
    "#train_data\n",
    "data_first_n=data[:partitions]\n",
    "#test_data\n",
    "data_last_n=data[partitions:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c899209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to apply MinMaxScaler.fit_transform and train_data set\n",
    "def fit_transform_train_data(data):\n",
    "    data.columns=[\"DateTime\",\"Price\"] #rename columns\n",
    "    \n",
    "    #set datetime columns as index\n",
    "    data.set_index(\"DateTime\",drop=True,inplace=True)\n",
    "    \n",
    "    #apply minmax scaler on price column   \n",
    "    data[data.columns]=mn.fit_transform(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to apply MinMaxScaler.transform on test_data_set\n",
    "def transform_test_data(data):\n",
    "    #rename columns\n",
    "    data.columns=[\"DateTime\",\"Price\"] \n",
    "    \n",
    "    #set datetime columns as index\n",
    "    data.set_index(\"DateTime\",drop=True,inplace=True)\n",
    "    \n",
    "    #apply minmax scaler on price column \n",
    "    data[data.columns]=mn.transform(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step is for processing inputs.\n",
    "#I am using previous 5 values to predict the present value . This can be changed as per your convenience\n",
    "previous_days=5\n",
    "def obtain_models_input(data):\n",
    "    #change to numpy\n",
    "    values=data[\"Price\"].to_numpy()\n",
    "    input_=[]\n",
    "    output_=[]\n",
    "    \n",
    "    #i am using past 5 values to predict the present value\n",
    "    for i in range(previous_days,len(values)):\n",
    "        \n",
    "        input_.append([values[i-previous_days:i]])\n",
    "        \n",
    "        output_.append([values[i]])\n",
    "        \n",
    "    #return in the form of numpy  \n",
    "    return np.array(input_),np.array(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a08d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply above functions on train and test datasets respectively\n",
    "data_first_n=fit_transform_train_data(data_first_n)\n",
    "\n",
    "data_last_n=transform_test_data(data_last_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data as per timestamp intervals as discussed above\n",
    "X_train,y_train=obtain_models_input(data_first_n)\n",
    "\n",
    "X_test,y_test=obtain_models_input(data_last_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shapes\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of LSTM Neural Network Model\n",
    "\n",
    "from keras.layers import Dense,LSTM,SimpleRNN,TimeDistributed,Input,Dropout,BatchNormalization\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(Input(shape=(1,previous_days)))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(30,return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(10,return_sequences=True))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c02226",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping=keras.callbacks.EarlyStopping(min_delta=0.1,patience=2,monitor=\"accuracy\",restore_best_weights=True)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss=\"mse\",optimizer=keras.optimizers.SGD(learning_rate=0.01),metrics=[\"accuracy\",\"mean_absolute_error\"])\n",
    "\n",
    "#Fit the model\n",
    "model.fit(X_train,y_train,epochs=80,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction function\n",
    "def model_predict(X_test):\n",
    "    #apply model.predict()\n",
    "    y_pred=model.predict(X_test)\n",
    "    #convert 3d array to 2d array\n",
    "    y_pred=y_pred.reshape(len(y_pred),1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=mn.inverse_transform(y_pred)\n",
    "y_test=mn.inverse_transform(y_test)\n",
    "fig, ax=plt.subplots(2)\n",
    "\n",
    "\n",
    "ax[0].plot(range(len(y_pred)),y_pred,color=\"orange\")\n",
    "ax[1].plot(range(len(y_test)),y_test,color=\"blue\")\n",
    "ax[0].set_title(\"Predicted\")\n",
    "ax[1].set_title(\"True Value\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from AWS S3 bucket for real time predictions\n",
    "def read_data_from_s3_bucket(last_read_file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket('your-bucket-name')\n",
    "\n",
    "    DateTime,Price=[],[]\n",
    "    key_list=[]\n",
    "    \n",
    "    print(last_read_file)\n",
    "    #We need to store the file names \n",
    "    for obj in bucket.objects.all():\n",
    "        key = obj.key\n",
    "        #read the files which are not read previously or use values which are greater than last analyzed values\n",
    "        if(key>last_read_file):\n",
    "            \n",
    "            print(key)\n",
    "            key_list.append(key)\n",
    "            \n",
    "            #Read the json message\n",
    "            body = json.loads(obj.get()['Body'].read())\n",
    "            \n",
    "            #Store the respective fields in list so we can convert into dataframe\n",
    "            DateTime.append(body[\"DateTime\"])\n",
    "            Price.append(body[\"Price\"])\n",
    "            \n",
    "    #convert the list into dataframe \n",
    "    df1=pd.DataFrame([DateTime,Price]).T\n",
    "    \n",
    "    #return the dataframe and last read file name \n",
    "    return df1,key_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially last_read_file is null. It gets updated regularly\n",
    "last_read_file=\"\"\n",
    "\n",
    "# this loop runs indefinetly untill it is interrupted externally\n",
    "while True:\n",
    "    #receive the data frame and last read timestamp\n",
    "    df1,last_read_file=read_data_from_s3_bucket(last_read_file)\n",
    "    \n",
    "    #apply transformations on the dataframe\n",
    "    df1=transform_test_data(df1)\n",
    "    \n",
    "    #get data that can be fed into model\n",
    "    df1_train,df1_test = obtain_models_input(df1)\n",
    "    \n",
    "    #for my convenience in debugging purpouse\n",
    "    print(df1_train.shape)\n",
    "    print(df1_test.shape)\n",
    "    \n",
    "    #apply model.predict on the newly obtained data\n",
    "    y_pred1=model_predict(df1_train)\n",
    "    \n",
    "    print(y_pred1.shape)\n",
    "    \n",
    "    #print the real time values periodically\n",
    "    for p,q,r,s in zip(df1_test,y_pred1,mn.inverse_transform(df1_test),mn.inverse_transform(y_pred1)):\n",
    "        print(p,q,r,s)\n",
    "    \n",
    "    #run for every 10 minutes \n",
    "    time.sleep(600) #run after every 10 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
